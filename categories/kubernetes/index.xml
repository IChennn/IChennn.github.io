<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on Murmuring</title>
    <link>https://ichennn.github.io/categories/kubernetes/</link>
    <description>Recent content in Kubernetes on Murmuring</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Apr 2025 12:08:34 +0900</lastBuildDate><atom:link href="https://ichennn.github.io/categories/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kubernetes Pod Security Admission (PSA) 筆記與實測</title>
      <link>https://ichennn.github.io/blog/2025/04/kubernetes-pod-security-admission-psa-%E7%AD%86%E8%A8%98%E8%88%87%E5%AF%A6%E6%B8%AC/</link>
      <pubDate>Thu, 17 Apr 2025 12:08:34 +0900</pubDate>
      
      <guid>https://ichennn.github.io/blog/2025/04/kubernetes-pod-security-admission-psa-%E7%AD%86%E8%A8%98%E8%88%87%E5%AF%A6%E6%B8%AC/</guid>
      <description>自從換了新公司之後，因為業界的性質，安全性一直是在實務上一個非常大的考量。可以不做新功能，可以不上新版本，但一定要嚴格要求安全性。
什麼是Pod Security Admission (PSA)？ 先來說說什麼是Pod Security Standards（PSS）。
PSS簡單來說是一系列的predefined的security policy，根據某些安全基準要求必須在manifest當中定義某項設定，而該項設定又有嚴格的Allow值，除了允許的設定內容外都被視為不合格（包含未定義該項設定）。
而PSS根據嚴格程度從輕到重分成三種等級（LEVEL）：Privileged、Baseline、Restricted
詳細的說明及規範的內容都在官方的文檔裡有詳細介紹，在此就割愛不贅述了。
簡單介紹了PSS後，再回頭來說PSA。Pod Security Admission (PSA)是一個從kubernetes 1.25開始的功能，是一個build-in的controller，以簡單的labeling方式去實踐PSS。一般來說是以namespace為單位去apply，也可以對整個cluster去apply。
而根據pod被判定為non-compliant時的動作，可以分為三種模式（MODE）：enforce、audit、warn
在最嚴格的enforce模式下，若pod被判定不合規，會直接被rejected。
實際作法 說穿了其實超級單純，就是在目標ns或是cluster加上一個label pod-security.kubernetes.io/&amp;lt;MODE&amp;gt;: &amp;lt;LEVEL&amp;gt;
若只想限制特定namespace內的pod：
kubectl label ns xxxx pod-security.kubernetes.io/enforce=restricted
上述的缺點就是若後來創建了新namespace，很有可能就成為漏網之魚了。
若想一勞永逸對全體cluster都加上限制的話（目前公司使用Azure因此是aks的command）：
az aks update --resource-group xxx --name xxx --security-profile workload-policy=restricted
rollback的方式也很直觀，就是把label拿掉就行了
kubectl label ns test pod-security.kubernetes.io/enforce-
az aks update --resource-group xxx --name xxx --security-profile workload-policy=none
一些筆記 不知道該幫這個段落取什麼名字，哈哈哈
總之在嘗試的過程中，發現一些值得留意的點，在實務上可能會造成誤會及困擾，因此覺得該記錄一下以供日後參考。
non-compliant pod不會立刻被reject 當label被加上之後，若有violation的話會出現warning，詳細指出哪些設定不符規則，但現存的pod (running狀態下)並不會立刻受到影響。
pod依然可以正常運作，但是當下一次pod被update或是restart時則會被reject。因此PSA帶來的限制效果可能是有一定的時間延遲的。
也就是說PSA並不會溯及既往，只會影響發生在限制加上後的pod operation。
Validation kubernetes 1.30+後，有一種新的resource可以作為validation的方法，像個守門員？一樣替你檢查新做成的pod合不合規定。
因為檢查的條件可以自己定義，因此不局限於PSA的內容，任何關於pod的設定，例如最少要幾個replica之類的規則也都沒問題。
由於這次沒有實驗這個功能，就只留下document作為一個筆記了。</description>
    </item>
    
    <item>
      <title>Kubernetes cluster migration（verelo篇)</title>
      <link>https://ichennn.github.io/blog/2023/12/kubernetes-cluster-migrationverelo%E7%AF%87/</link>
      <pubDate>Tue, 12 Dec 2023 01:04:16 +0900</pubDate>
      
      <guid>https://ichennn.github.io/blog/2023/12/kubernetes-cluster-migrationverelo%E7%AF%87/</guid>
      <description>上篇：
https://ichennn.github.io/blog/2023/11/kubernetes-cluster-migrationkubectl%E7%AF%87/
雖然上一篇使用kubectl移植workload跟pv沒有什麼問題，但就是步驟略多略複雜，又是全部手動操作，移植的對象一多，難免曠日廢時忙中有錯ＸＤ
恩～有沒有更好更快的方法呢？
突然靈光一閃，之前我不是抱著嚐鮮的心態安裝了velero在cluster裡嗎！一直以來沒什麼實際應用的機會略感可惜，這不，天上掉下來一個大好的機會，正好可以試試看velero好不好用。
安裝velero 其實安裝沒碰到太多的困難，只要準備好存放備份的object storage，接下來照著公式上的步驟deploy到cluster上就行了。
https://velero.io/docs/v1.12/
velero雖然看似單純的一個小工具，其實用途很多，像是disaster recovery、cluster migration、或是日常的測試時用來回復備份等等，沒有不會怎樣，但有了之後突然感覺很方便的概念。
由於不是本篇的重點，公式網站的文章也有非常詳細的說明，因此就不贅述了。
使用velero來備份 # velero backup create --from-schedule stg-k8s-backup INFO[0000] No Schedule.template.metadata.labels set - using Schedule.labels for backup object backup=velero/stg-k8s-backup-20230210160910 labels=&amp;#34;map[]&amp;#34; Creating backup from schedule, all other filters are ignored. Backup request &amp;#34;stg-k8s-backup-20230210160910&amp;#34; submitted successfully. Run `velero backup describe stg-k8s-backup-20230210160910` or `velero backup logs stg-k8s-backup-20230210160910` for more details. # velero backup get | grep stg-jpe12-k8s-backup-20230210 stg-k8s-backup-20230210160910 Completed★ 0 1 2023-02-10 16:09:11 +0900 JST 6d default &amp;lt;none&amp;gt; [舊cluster]將deployment replica=0 這邊和上一篇是一樣的，主要是想要保險一點，在replica=0的狀態下移植到新的環境，避免ip衝突，或是其他有可能的紕漏。</description>
    </item>
    
    <item>
      <title>Kubernetes cluster migration（kubectl篇）</title>
      <link>https://ichennn.github.io/blog/2023/11/kubernetes-cluster-migrationkubectl%E7%AF%87/</link>
      <pubDate>Mon, 06 Nov 2023 23:04:22 +0900</pubDate>
      
      <guid>https://ichennn.github.io/blog/2023/11/kubernetes-cluster-migrationkubectl%E7%AF%87/</guid>
      <description>因為EOL的緣故要更新k8s的vm OS，順便將已經很久沒有更新的weave-net換成calico，因此決定重新建一個cluster，再把原本的workload轉移到新的cluster上。
乍聽之下沒什麼問題，畢竟原本就是用manifest管理的，平時更新workload也都是重新將manifest deploy一遍就行。
但真正的問題在於，其中一些workload，像是jenkins，是將app的資料儲存在pv裡，若要轉移到新的cluster上，也要連帶著將pv裡的資料也轉移過去才行。
因此如何轉移pv變成了這次migration最大的課題。
首先梳理一下cluster的狀況：
以jenkins為例： deployment -&amp;gt; (經由nfs provisioner創建) pvc -&amp;gt; pv -&amp;gt; nfs https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner 作為storage的nfs沒有改變 也就是說在nfs上舊cluster用來儲存資料的path不會移動或改變，只是怎麼讓新cluster使用這個路徑作為pv的問題 移植pv與pvc的方法是參考這篇文章實作的：
https://acv.engineering/posts/migrating-a-kubernetes-pvc/
[新cluster] Deploy NFS provisioner 什麼是NFS provisioner呢？這是一個由kubernetes-sigs提供的工具，全稱是Kubernetes NFS Subdir External Provisioner。
可以非常方便的使用既有的NFS server作為pv的儲存路徑，而pv名稱會以${namespace}-${pvcName}-${pvName}的格式呈現。
由於舊cluster上的pv都是透過nfs provisioner去創建的，因此在轉移之前，必須先在新的cluster上也deploy NFS provisioner。
# kubectl apply -k jenkins-nfs-provisioner/test/ storageclass.storage.k8s.io/jenkins-nas-nfs created serviceaccount/jenkins-nfs-client-provisioner created role.rbac.authorization.k8s.io/jenkins-leader-locking-nfs-client-provisioner created clusterrole.rbac.authorization.k8s.io/jenkins-nfs-client-provisioner-runner created rolebinding.rbac.authorization.k8s.io/jenkins-leader-locking-nfs-client-provisioner created clusterrolebinding.rbac.authorization.k8s.io/jenkins-nfs-client-provisioner created deployment.apps/jenkins-nfs-client-provisioner created # kubectl get all | grep nfs pod/jenkins-nfs-client-provisioner-69978cf7f4-vzs2g 1/1 Running 0 118s deployment.apps/jenkins-nfs-client-provisioner 1/1 1 1 118s replicaset.</description>
    </item>
    
    <item>
      <title>CKA備考＆常用指令筆記</title>
      <link>https://ichennn.github.io/blog/2023/08/cka%E5%82%99%E8%80%83%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%E7%AD%86%E8%A8%98/</link>
      <pubDate>Sat, 12 Aug 2023 17:29:03 +0900</pubDate>
      
      <guid>https://ichennn.github.io/blog/2023/08/cka%E5%82%99%E8%80%83%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%E7%AD%86%E8%A8%98/</guid>
      <description>最初因為工作需要而開始接觸Kubernetes，組內的前輩們花了一段時間建立了一個cluster，有好幾個worker node，上面也已經deployed了各種app。
身為一個剛被配屬的小菜鳥，也從來沒有接觸過k8s，前輩們於是決定幫我開一個ナレッジ共有会，從pod的基本概念講起，淘淘不絕分享了兩小時（其實仔細想想，線上課都要20幾小時，兩小時要是我就聽懂了也是滿神的），但是只換來我呆滯的眼神跟停止思考的腦袋&amp;hellip;&amp;hellip;
總而言之，就是一知半解，但工作還是要做，日子還是要過。我就這樣半推半就（？）進入了kubernetes的世界，竟也跌跌撞撞但有驚無險的摸懂了基本操作，甚至還主導了幾次cluster upgrade跟cluster migration。幾句話的輕描淡寫，背後也是花了好幾年的時間，期間一直在想，我應該要找個線上課老老實實地從頭開始打基本功，能順便考個CKA就好了。
但行動力像條蟲的我，拖延病一發作就是一兩年，直到今年，各種層面上想要好好整頓一下自己的生活，也包括未來的職涯。也因此好好把CKA拿到手，就又回到我的待辦事項中，趁著4月新生活打折的時機，以30%off的價格買了CKA考試。
線上課程 跟網路上大部分人一樣，都是找了udemy上Mumshad Mannambeth的Certified Kubernetes Adminidtrator(CKA) with Practice Tests來看。
不得不說，真的清楚易懂，而且每個小單元都有練習題，最後還有三個模擬試題可以練習，雖然環境和真正考試環境並不是100%相同，但也相去不遠了。課程影片和hands on lab雖然是相輔相成的，但畢竟考試的形式不是選擇題，而是上機實際操作，hands on lab其實才是決定能不能順利通過考試的關鍵！！
說實在，若有無限的時間慢慢google的話其實都不是難事（畢竟是開書考ＸＤ），但難就難在必須在有限的時間內（2小時）完成17道題目，而且要敲的指令也不算少，做完題目也得檢查一下比較保險，因此把簡單的指令記下來，並知道要怎麼在doc中搜尋出複雜的指令就成了考試對策中一個重要的環節。（大概就跟考多益一樣，到最後也不見得是在考英文能力，而是耐心跟讀題速度ＸＤ）
kubectl常用指令 雖然說是為了考試而寫的筆記，但在自己過去的實務經驗上，好用實用的指令還是很多的，準備CKA也不只是為了那張證書，在自己的技術提升上也是受益良多了。
參考Document url CKA開書考可參考範圍 https://kubernetes.io/docs/home/ k8s cheat sheet https://kubernetes.io/docs/reference/kubectl/cheatsheet/ 指令 kubectl help https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands 前置設定 生成yaml
export do=&amp;#34;--dry-run=client -o yaml&amp;#34; create各種 Pod
雖然被歸類在create，但生成pod是唯一不能使用kubectl create的
kubectl run &amp;lt;pod-name&amp;gt; 生成1個以上container的pod
先create一個container，再手動修改yaml Deployment, Daemonset
Deployment
kubectl create deployment &amp;lt;name&amp;gt; --image=xxx --replicas=3 (-r 3) Daemonset
先create deployment，再手動將Kind: Deployment改成Kind: Daemonset Secret
docker-registry
kubectl create secret docker-registry xxx --docker-server=xxx:5000 --docker-username=user --docker-password=pass generic</description>
    </item>
    
  </channel>
</rss>
